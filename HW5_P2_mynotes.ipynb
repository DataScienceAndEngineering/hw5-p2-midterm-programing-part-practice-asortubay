{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###HOMEWORK 5 - PART 2: FASHION MNIST EXAMPLE CLASSIFICATION USING PYTORCH###\n",
        "\n",
        "FROM: https://www.kaggle.com/code/pankajj/fashion-mnist-with-pytorch-93-accuracy/notebook"
      ],
      "metadata": {
        "id": "mOnXBREP0zT6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-DmVmhN0xqx"
      },
      "source": [
        "### This is the tutorial of deep learning on FashionMNIST dataset using Pytorch. We will build a Convolutional Neural Network for predicting the classes of Dataset. I am assuming you know the basics of deep leanrning like layer architecture... convolution concepts. Without further ado... Lets start the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rm65OztSCpF"
      },
      "source": [
        "# **Importing Important Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy, pandas, matplotlib, torch, torch.nn, torch.autograd as Variable, also torchvision, torchvision(transforms), Dataset and Dataloader, and confusion matrix from sklearn.metrics."
      ],
      "metadata": {
        "id": "BAHltRoT02WB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-1_8VZgpEtea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6bjL6wESfoV"
      },
      "source": [
        "### If the GPU is available use it for the computation otherwise use the CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VePPK186m6iS"
      },
      "source": [
        "### 2. Using FashionMNIST class from torchvision module.\n",
        "\n",
        "\n",
        "*   It will download the dataset first time, train=False for test_set and transform to tensor the input images. Define train_set and test_set\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM618_wYGM0n",
        "outputId": "6eba3133-5f65-4792-f09a-00b65107a3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8140147.22it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 139421.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2551442.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 6068655.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then generate the train_loader and test_loader using Dataloader, and a batch_size of 100"
      ],
      "metadata": {
        "id": "NosZGPyA2e2G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8s2uhlGJZOOP"
      },
      "outputs": [],
      "source": [
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SfsxOX4peHU"
      },
      "source": [
        "### We have 10 types of clothes in FashionMNIST dataset.\n",
        "\n",
        "\n",
        "> Making a method that return the name of class for the label number.\n",
        "ex. if the label is 5, we return Sandal.\n",
        "\n",
        "\n",
        "\n",
        "> The method is names 'output_label', and is a mapping between numbers and labels:\n",
        "                 0: \"T-shirt/Top\",\n",
        "                 1: \"Trouser\",\n",
        "                 2: \"Pullover\",\n",
        "                 3: \"Dress\",\n",
        "                 4: \"Coat\",\n",
        "                 5: \"Sandal\",\n",
        "                 6: \"Shirt\",\n",
        "                 7: \"Sneaker\",\n",
        "                 8: \"Bag\",\n",
        "                 9: \"Ankle Boot\"\n",
        "\n",
        "\n",
        "\n",
        "> add this so input is what is expected: \"input = (label.item() if type(label) == torch.Tensor else label)\n",
        "    return output_mapping[input]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWIE3hVqOlMi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDH7i5UFo7w3"
      },
      "source": [
        "### Playing with data and displaying some images using matplotlib imshow() method.\n",
        "\n",
        "Get first item in train_dataloader, show its size. Plot size of training set, and also show the image and label for first item in train_dataloader, also show batch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB9jenaDYZmt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-PnIoRpjuZW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kC6CrJrlbf_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgpYsgh0PY09"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z0D4BgQRW8e"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM2u0Riup9mx"
      },
      "source": [
        "## Building a CNN\n",
        "\n",
        "\n",
        "*   Make a model class (FashionCNN in our case)\n",
        "    * It inherit nn.Module class that is a super class for all the neural networks in Pytorch.\n",
        "* Our Neural Net has following layers:\n",
        "    * Two Sequential layers each consists of following layers-\n",
        "        * Convolution layer that has kernel size of 3 * 3, padding = 1 (zero_padding) in 1st layer and padding = 0 in second one. Stride of 1 in both layer.\n",
        "        * Batch Normalization layer.\n",
        "        * Acitvation function: ReLU.\n",
        "        * Max Pooling layer with kernel size of 2 * 2 and stride 2.\n",
        "     * Flatten out the output for dense layer(a.k.a. fully connected layer).\n",
        "     * 3 Fully connected layer  with different in/out features.\n",
        "     * 1 Dropout layer that has class probability p = 0.25.\n",
        "  \n",
        "     * All the functionaltiy is given in forward method that defines the forward pass of CNN.\n",
        "     * Our input image is changing in a following way:\n",
        "        * First Convulation layer : input: 28 \\* 28 \\* 3, output: 28 \\* 28 \\* 32\n",
        "        * First Max Pooling layer : input: 28 \\* 28 \\* 32, output: 14 \\* 14 \\* 32\n",
        "        * Second Conv layer : input : 14 \\* 14 \\* 32, output: 12 \\* 12 \\* 64\n",
        "        * Second Max Pooling layer : 12 \\* 12 \\* 64, output:  6 \\* 6 \\* 64\n",
        "    * Final fully connected layer has 10 output features for 10 types of clothes.\n",
        "\n",
        "> Lets implementing the network...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyCH0Q4hSgFB"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mMVaW_PvCC2"
      },
      "source": [
        "### Making a model of our CNN class\n",
        "\n",
        "*   Creating a object(model in the code)\n",
        "*   Transfering it into GPU if available.\n",
        "*  Defining a Loss function. we're using CrossEntropyLoss() here.\n",
        "*  Using Adam algorithm for optimization purposes, and use learning reate of 0.001\n",
        "* Print the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NILDHzNgQ1Gt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im58ZlvMvkty"
      },
      "source": [
        "## Training a network and Testing it on test dataset\n",
        "\n",
        "\n",
        "\n",
        "*   5 epochs\n",
        "*   Generate lists for loss, iteration and accuracy (for visualization)\n",
        "*   Also predictions list and labels list\n",
        "* Start loop over epochs:\n",
        "  *   Loop over mini-batches in train_loader (count mini-batch iterations)\n",
        "    *   transfer images and labels to device\n",
        "    *   shape mini-batch in expected tensor format\n",
        "    *   Do forward pass\n",
        "    *   Compute loss from error (crossentropyloss)\n",
        "    *   Set gradients to 0 for current batch\n",
        "    *   Backrpropragate error\n",
        "    *   Optimize the parameters (step)\n",
        "  *   If iteration % 50 == 0\n",
        "    *   Iterate over the batches of images and labels in the test data loader.\n",
        "    *   Move the images and labels to the specified device\n",
        "    *   Append the true labels to the labels_list for future use\n",
        "    *   Pass the test images through the model to get the predicted outputs\n",
        "    *   Extract the predicted labels by taking the index of the maximum value along the appropriate axis and move them to the device\n",
        "    *   Append the predicted labels to predictions_list\n",
        "    *   Calculate the number of correct predictions by summing the number of correct matches between predicted and true labels\n",
        "    *   Increment the total count by the number of labels in the batch\n",
        "    *   Calculate the accuracy as the percentage of correct predictions over the total number of samples\n",
        "    *   Store metrics: Append the loss value to loss_list, current iteration count to iteration_list, accuracy to accuracy_list\n",
        "  *   If iteration % 50 == 0\n",
        "    *   print the iteration count, loss, and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYh_6HtpUlNl"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xm9MZ6O0irC"
      },
      "source": [
        "### Visualizing the Loss and Accuracy with Iterations\n",
        "plot iterations vs loss and iterations vs accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s4cGX4Hanyz"
      },
      "outputs": [],
      "source": [
        "plt.plot(iteration_list, loss_list)\n",
        "plt.xlabel(\"No. of Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Iterations vs Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtsjkmY8qo_t"
      },
      "outputs": [],
      "source": [
        "plt.plot(iteration_list, accuracy_list)\n",
        "plt.xlabel(\"No. of Iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Iterations vs Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZTkzUKn0vq3"
      },
      "source": [
        "### Looking the Accuracy in each class of FashionMNIST dataset\n",
        "\n",
        "run over mini-batches in test_loader, run model and save output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq9_qes8Qg6h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxZhix0v0_KZ"
      },
      "source": [
        "### Printing the Confusion Matrix\n",
        "\n",
        "get list from the predictions_lsit and labels_list. use sklearns' confusion_matrix to plot it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXme4c22cii2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft-Qlbb5bl0A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y9pGNv64UYl"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}